\documentclass[12pt,twoside]{article}

\newcommand{\reporttitle}{495 Advanced Statistical Machine Learning and Pattern Recognition}
\newcommand{\reportauthor}{Thomas Teh}
\newcommand{\reporttype}{Notes}
\newcommand{\cid}{0124 3008}

% include files that load packages and define macros
\input{includes} % various packages needed for maths etc.
\input{notation} % short-hand notation and macros


%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
% front page
\input{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%% Main document
\section{Expectation Maximization}

\subsection{General Approach for Expectation Maximization}
\subsubsection{Notes}
\begin{enumerate}
\item The goal of the Expectation-Maximization is to find maximum likelihood solutions for models having latent variables
\item The general concept is that since our knowledge of the latent variables in $\vec{Z}$ is given by the posterior distribution $p(\vec{Z}\vert \vec{Z}, \vec{\theta})$, we use the expectation of the latent variables instead of the actual values.
\item EM algorithm can be used to find MAP solutions models in which a prior is defined over the parameters.
\end{enumerate}

\subsubsection{Algorithm}
Given a joint distribution $p(\vec{X}, \vec{Z}\vert \vec{\theta})$ over observed variables $\vec{X}$ and latent variables $\vec{Z}$, governed by parameter $\vec{\theta}$ the goal is to maximize the likelihood function $p(\vec{X}\vert \vec{\theta})$ w.r.t $\vec{\theta}$.
\begin{enumerate}
\item Choose an initial for the parameters $\vec{\theta}^{old}$.
\item E Step: Evaluate $p(\vec{Z} \vert \vec{X}, \vec{\theta}^{old})$
\item M Step: Evaluate $\vec{\theta}^{new}$ given by
\begin{align*}
\vec{\theta}^{new}& = \text{arg} \max_{\vec{\theta}} \mathcal{Q}(\vec{\theta},\vec{\theta}^{old})
\end{align*}
where
\begin{align*}
\mathcal{Q}(\vec{\theta},\vec{\theta}^{old})& = \sum_{\vec{z}}p(\vec{Z}\vert \vec{X}, \vec{\theta}^{old})\ln p(\vec{X}, \vec{Z} \vert \vec{\theta})
\end{align*}
\item Check for convergence of either the log-likelihood or the parameter values. If convergence is not satisfied
\begin{align*}
\vec{\theta}^{old} \leftarrow \vec{\theta}^{new}
\end{align*}
and return to step 2
\end{enumerate}


\subsection{Gaussian Mixture Models}
The Gaussian mixture distribution can be written as a linear superposition of Gaussians
\begin{align*}
p(\vec{x}) &= \sum_{k=1}^K \pi_k \mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k)
\end{align*}


\subsubsection{Formulation of the Gaussian Mixture Models}
Let $\vec{z}$ be a $K$-dimensional binary random variable with 1-of-$K$ representation.
\begin{align*}
p(z_k=1) = \pi_k 	&\Rightarrow p(\vec{z})=\prod_{i=1}^K \pi_k^{z_k}
\end{align*}

Conditional probability of $\vec{x}$ given a particular value for latent variable $\vec{z}$:
\begin{align*}
p(\vec{x}\vert \vec{z}) = \prod_{k=1}^K \mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k)^{z_k}
\end{align*}

Using Bayes theorem and marginalize the latent variable $\vec{z}$
\begin{align*}
p(\vec{x}) =\sum_{\vec{z}}p(\vec{z})p(\vec{x}\vert \vec{z}) = \sum_{k=1}^K \pi_k\mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k)
\end{align*}

Similarly, the posterior probability of $\vec{z}$ is given by
\begin{align*}
\gamma (z_k) = \frac{p(\vec{x}\vert \vec{z})p(\vec{z})}{p(\vec{x})} = \frac{\pi_k \mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k) }{\sum_{j=1}^K \pi_j\mathcal{N}(\vec{x}\vert \vec{\mu}_j, \vec{\Sigma}_j)}
\end{align*}

\subsubsection{Maximum Likelihood}
The log-likelihood function is given by
\begin{align*}
\ln p(\vec{X} \vert \vec{\pi}, \vec{\mu}, \vec{\Sigma})&=\sum_{n=1}^N \ln\left(\sum_{k=1}^K \pi_k\mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k)\right)
\end{align*}

The maximum likelihood method will yield the following:
\begin{align*}
\vec{\mu}_k & = \frac{\sum_{n=1}^N \gamma(z_{nk})\vec{x}_n}{\sum_{n=1}^N \gamma(z_{nk})}\\
\vec{\Sigma}_k& = \frac{\sum_{n=1}^N \gamma(z_{nk})(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^\top}{\sum_{n=1}^N \gamma(z_{nk})}\\
\pi_k&= \frac{\sum_{n=1}^N \gamma(z_{nk})}{N}
\end{align*}

Issues with maximum likelihood:
\begin{enumerate}
\item Presence of singularities: When we have at least two components in the mixture, one of them can have a finite variance and assign finite probability to all the data points, while the other component can shrink onto one specific data point and therefore contribute to an ever increasing additive value to the log likelihood.
\item Identifiability: Solutions may not be unique, hence it may be hard to interpret the parameter values discovered by a model.
\item The log likelihood equation is difficult to optimize over.
\end{enumerate}


\subsubsection{Expectation Maximization Formulation}
Supposed that in addition to $\vec{X}$, we were also given the values of the latent variables $\vec{Z}$, the likelihood and the log likelihood function are given by
\begin{align*}
p(\vec{X}, \vec{Z}\vert \vec{\mu}, \vec{\Sigma}, \vec{\pi})& =\prod_{n=1}^{N}\prod_{k=1}^{K} \pi_k^{z_{nk}}\mathcal{N}(\vec{x}_n\vert \vec{\mu}_k, \vec{\Sigma}_k)^{z_{nk}}\\
\ln p(\vec{X}, \vec{Z}\vert \vec{\mu}, \vec{\Sigma}, \vec{\pi})& =\sum_{n=1}^{N}\sum_{k=1}^{K} z_{nk}\left(\ln \pi_k + \ln \mathcal{N}(\vec{x}_n\vert \vec{\mu}_k, \vec{\Sigma}_k\right)
\end{align*}

\begin{enumerate}
\item \textbf{Expectation Step}:

Taking the expectation on the log likelihood 
\begin{align*}
\mathbb{E}_{p(\vec{Z}\vert \vec{X}, \vec{\theta})}\left[\ln p(\vec{X}, \vec{Z}\vert \vec{\mu}, \vec{\Sigma}, \vec{\pi})\right]
& =\sum_{n=1}^{N}\sum_{k=1}^{K} \mathbb{E}_{p(\vec{Z}\vert \vec{X}, \vec{\theta})}[z_{nk}]\left(\ln \pi_k + \ln \mathcal{N}(\vec{x}_n\vert \vec{\mu}_k, \vec{\Sigma}_k\right)\\
& = G(\vec{\theta})
\end{align*}

\begin{align*}
p(\vec{Z}\vert \vec{X}, \vec{\theta})\propto &
\prod_{n=1}^N\prod_{k=1}^K [\pi_k\mathcal{N}(\vec{x}_n\vert \vec{\mu}_k, \vec{\Sigma}_k)]^{z_{nk}}
\end{align*}

\begin{align*}
\mathbb{E}_{p(\vec{Z}\vert \vec{X}, \vec{\theta})}[z_{nk}]
&= \frac{\sum_{z_{nk}}z_{nk}[\pi_k\mathcal{N}(\vec{x}_n\vert \vec{\mu}_k, \vec{\Sigma}_k)]^{z_{nk}} }{\sum_{j=1}^K \pi_j \mathcal{N}(\vec{x}_n \vert \vec{\mu}_j, \vec{\Sigma}_j)}\\
&= \frac{\pi_k \mathcal{N}(\vec{x}_n \vert \vec{\mu}_k, \vec{\Sigma}_k)}{\sum_{j=1}^K \pi_j \mathcal{N}(\vec{x}_n \vert \vec{\mu}_j, \vec{\Sigma}_j)}\\
&=\gamma({z_{nk}})
\end{align*}

\item \textbf{Maximization Step}:
By taking the derivative of $(\vec{\theta})$ w.r.t $\vec{\theta}$ and set them to $0$, the parameters can be found to be
\begin{align*}
\vec{\mu}_k & = \frac{\sum_{n=1}^N \gamma(z_{nk})\vec{x}_n}{\sum_{n=1}^N \gamma(z_{nk})}\\
\vec{\Sigma}_k& = \frac{\sum_{n=1}^N \gamma(z_{nk})(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^\top}{\sum_{n=1}^N \gamma(z_{nk})}\\
\pi_k&= \frac{\sum_{n=1}^N \gamma(z_{nk})}{N}
\end{align*}

\end{enumerate}

\subsubsection{Expectation Maximization Algorithm}
Given a Gaussian mixture model, the goal is to maximize the likelihood functions w.r.t to the parameters:
\begin{enumerate}
\item Initialize the means $\vec{\mu}_k$, covariances $\vec{\Sigma}_k$ and mixing coefficients $\pi_k$ and evaluate the initial of the log likelihood.
\item E Step: Evaluate the responsibilities using the current parameter values
\begin{align*}
\gamma (z_k) = \frac{p(\vec{x}\vert \vec{z})p(\vec{z})}{p(\vec{x})} = \frac{\pi_k \mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k) }{\sum_{j=1}^K \pi_j\mathcal{N}(\vec{x}\vert \vec{\mu}_j, \vec{\Sigma}_j)}
\end{align*}
\item M Step: Re-estimate the parameters using the current responsibilities
\begin{align*}
\vec{\mu}_k^{new}&=\frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})\vec{x}_n\\
\vec{\Sigma}_k^{new}&=\frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})(\vec{x}_n-\vec{\mu}_k)(\vec{x}_n-\vec{\mu}_k)^\top\\
\pi_k^{new}&=\frac{N_k}{N}
\end{align*}
where
\begin{align*}
N_k & = \sum_{n=1}^N \gamma(z_{nk})
\end{align*}
\item Evaluate the log likelihood
\begin{align*}
\ln p(\vec{X} \vert \vec{\pi}, \vec{\mu}, \vec{\Sigma})&=\sum_{n=1}^N \ln\left(\sum_{k=1}^K \pi_k\mathcal{N}(\vec{x}\vert \vec{\mu}_k, \vec{\Sigma}_k)\right)
\end{align*}
and check for convergence of either parameter of the log likelihood. If the convergence criterion is not satisfied, return to step 2
\end{enumerate}


\subsection{Bernoulli Mixture Models}

\begin{align*}
p(\vec{x}\vert \vec{\mu}) & = \prod_{i=1}^D \mu_i^{x_i}(1-\mu_i)^{1-x_i}\\
\mathbb{E}[\vec{x}] & = \vec{\mu}\\
\mathbb{V}[\vec{x}] &= diag\lbrace \mu_i(1-\mu_i)\rbrace
\end{align*}

The mixture of the Bernoulli distributions is given by
\begin{align*}
p(\vec{x}\vert \vec{\mu}, \vec{\pi})&=\sum_{k=1}^K \pi_kp(\vec{x}\vert \vec{\mu}_k)\\
p(\vec{x}\vert \vec{\mu}_k) &= \prod^{D}_{i=1} \mu_{ki}^{x_i} (1- \mu_{ki}^{x_i})^{1-x_i}\\
\mathbb{E}[\vec{x}] & = \sum_{k=1}^K \pi_k\vec{\mu}_k\\
\mathbb{V}[\vec{x}] &= \sum_{k=1}^K \pi_k \left(\vec{\Sigma}_k + \vec{\mu}_k\vec{\mu}_k^\top\right) - \mathbb{E}[\vec{X}]\mathbb{E}[\vec{X}]^\top\\
\vec{\Sigma}_k & = diag\lbrace \mu_{ki}(1-\mu_{ki})\rbrace
\end{align*}

Let $\vec{z}$ be the one hot representation, we have
\begin{align*}
p(\vec{x} \vert \vec{z}, \vec{\mu}) &= \prod_{k=1}^K p(\vec{x}\vert \vec{\mu}_k)^{z_k}\\
p(\vec{z}\vert \vec{\pi}) &= \prod_{k=1}^K \pi_k^{z_k}
\end{align*}

The log likelihood function is given by
\begin{align*}
\ln p(\vec{X}, \vec{Z} \vert \vec{\mu}, \vec{\pi})&=\sum_{n=1}^N\sum_{k=1}^Kz_{nk}
\left\lbrace 
\ln \pi_k +\sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]
\right\rbrace
\end{align*}

The E step: we then take the expectation of the log likelihood above
\begin{align*}
\mathbb{E}_{\vec{Z}}[\ln p(\vec{X}, \vec{Z} \vert \vec{\mu}, \vec{\pi})]&=\sum_{n=1}^N\sum_{k=1}^K\gamma(z_{nk})
\left\lbrace 
\ln \pi_k +\sum_{i=1}^D[x_{ni}\ln\mu_{ki}+(1-x_{ni})\ln(1-\mu_{ki})]
\right\rbrace\\
&\\
\gamma(z_{nk})& = \frac{\sum_{z_{nk}} z_{nk}[\pi_kp(\vec{x}_n\vert \vec{\mu}_k)]^{z_{nk}} }{\sum_{z_{nj}}[\pi_jp(\vec{x}_n\vert \vec{\mu}_j)]^{z_{nj}}}
=\frac{\pi_kp(\vec{x}_n\vert \vec{\mu}_k)}{\sum_{j=1}^K \pi_jp(\vec{x)_n}\vert \vec{\mu}_j)}
\end{align*}

The M step: we then maximize the log likelihood wrt to the parameters
\begin{align*}
\vec{\mu}_k &=\frac{1}{N_k}\sum_{n=1}^N \gamma(z_{nk})\\
\pi_k&=\frac{N_k}{N}\\
N_k &= \sum_{n=1}^N \gamma(z_{nk})
\end{align*}

\subsection{Convergence of the EM Algorithm}

\begin{align*}
p(\vec{X} \vert \vec{\theta}) = \sum_{\vec{Z}} p(\vec{X},\vec{Z} \vert \vec{\theta}) &\Rightarrow \ln p(\vec{X} \vert \vec{\theta}) = \mathcal{L}(q, \vec{\theta}) + KL(q\parallel p)\\
\mathcal{L}(q, \vec{\theta})&=\sum_{\vec{Z}} q(\vec{Z})\ln \left[\frac{p(\vec{X}, \vec{Z}\vert \vec{\theta})}{q(\vec{Z})}\right]\\
 KL(q\parallel p)&=\sum_{\vec{Z}} q(\vec{Z})\ln \left[\frac{p(\vec{Z}\vert \vec{X}, \vec{\theta})}{q(\vec{Z})}\right]
\end{align*}

In the E step, the lower bound $\mathcal{L}(q,\vec{\theta}^{old})$ is maximized with respect to $q(\vec{Z})$. The solution to this maximization problem can be done by setting $q(\vec{Z}) = p (\vec{Z}\vert \vec{X}, \vec{\theta}^{old})$ such that $KL(q\parallel p)=0$.\\

In the M step, the lower bound $\mathcal{L}(q,\vec{\theta}^{old})$ is maximized with respect to $\vec{\theta}$. However, the KL divergence will no longer be zero. Hence, the increase in the upper bound of the log likelihood function is greater than the increase in the lower bound.\\

By substituting $q(\vec{Z}) = p(\vec{Z}\vert\vec{X}, \vec{\theta}^{old})$, we get the lower bound to be the following after the E step:
\begin{align*}
\mathcal{L}(q,\vec{\theta}) & = \sum_{\vec{Z}}p(\vec{Z}\vert \vec{X}, \vec{\theta}^{old})\ln p(\vec{X},\vec{Z}\vert \vec{\theta}) - \sum_{\vec{Z}}p(\vec{Z}\vert \vec{X}, \vec{\theta}^{old})\ln p(\vec{Z} \vert \vec{X}, \vec{\theta}^{old})\\
&= \mathcal{Q}(\vec{\theta},\vec{\theta}^{old} ) + const 
\end{align*}


\section{Maximum Likelihood PCA}






\end{document}
%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
