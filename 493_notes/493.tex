\documentclass[12pt,twoside]{article}

\input{includes}
\input{notation}

\iffalse
\begin{figure}[tb]
\centering
\includegraphics[width = 0.7\hsize]{./figures/filename}
\caption{This is a figure.} 
\label{fig:} 
\end{figure}
\fi

\begin{document}

\section{Notes}
\begin{itemize}
    \item Unlike previous courses, it seems like our professor uses (sometimes bold, sometimes not) capitals for vectors, bold capitals for matrices, and unweighted lowercase characters for scalars, but in these notes I use the regular notation where vectors are capital lowercase.
\end{itemize}
\section{Bayes' Theorem and Bayesian Inference}

\begin{itemize}
    \item Bayesian inference refers to the process of calculating the posterior probability using Bayes' Theorem.
    \item Some mumbo jumbo about the relationships between prior/likelihood and subjective/objective assessment.
\end{itemize}

\section{Simple Bayesian Networks}

\begin{itemize}
    \item Link matrix:
        \begin{align}
            \mat{P(c|p)} = \begin{bmatrix} P(c_1|p_1) & P(c_1|p_2) \\ P(c_2|p_1) & P(c_2|_2) \end{bmatrix}
        \end{align}
    \item \textbf{Instantiating} a node refers to setting it to a measured value. For instance, if you observe a data point where \(c=1\), you may say \(c\) was \textit{instantiated} to 1 for that point.
    \item \textbf{Bayesian classifiers} may introduce more complex tree structures than the \textit{naive} ones.
    \item Develops intuition for a method that is often intractable.
\end{itemize}

\section{Evidence and Message Passing}

\begin{itemize}
    \item New concepts are necessary to deal with complex networks with intermediate nodes:
        \begin{itemize}
            \item \textbf{Evidence} is information we have at a node -- this may be gathered through instantiation (exact values or virtual evidence), or inferred from passing messages. Evidence is unnormalised probabilities so the absolute values are meaningless, but they are useful for making comparisons.
            \item \textbf{Messages} are information (evidence) passed between nodes to provide evidence to another node.
        \end{itemize}
    \item This lecture assumes each node has \textbf{one} parent.
    \item \textbf{Evidence} (unnormalised probability)
    \begin{itemize}
        \item \(\pi\) evidence generalises prior probability and comes from parents.
        \item \(\lambda\) evidence generalises likelihood and comes from children.
        \item Mnemonic: \textbf{p}i (\(\pi\)), \textbf{p}rior, and ``\textbf{p}arent'' all start with letter ``p''; and \textbf{l}ambda (\(\lambda\)), \textbf{l}ikelihood, and ``\textbf{l}ad'' all start with ``l''. Further, prior comes \textit{before} so from parents.
    \end{itemize}
    \item \textbf{\(\lambda\) evidence}
    \begin{itemize}
        \item \( \vec{\lambda(\vec{c})} = [\lambda(c_1), \lambda(c_2), \dots \lambda(c_n)] \)
        \item \begin{align}
                \lambda(c_i) = \prod_{h=\text{children of }c_i}\sum_{j=\text{indices for }h}\lambda(h_j)P(h_j|c_i) \stackrel{Vectorised}{=} \mat{\lambda(h)}\mat{P(h|c)}
                \end{align}
            \item Note that if any of the leaf notes \(h_i\) is not instantiated (i.e. we have no data/evidence on it), we can consider \(\lambda(h_i) =1\) (for all \(i\)), thus it's a sum of probabilities which is equal to 1. If a value is known, then only \(\lambda(h_j) =1\) where \(h_j\) is the known value, and in the case of virtual evidence, \(\lambda(h_j)\) is given by the distribution chosen.
            \item If you just want to find the root (hypothesis) node, then you can calculate it just from \(\lambda\) evidence as they all go up.
    \end{itemize}
    \item \textbf{\(\pi\) evidence}
        \begin{itemize}
            \item This is useful when one wants to calculate the evidence of a node whose parents are not instantiated, but there is some evidence about the state of their parents from other siblings.
            \item \begin{align}
                    \pi(c_i) = \sum_{p_j=\text{values of parent}}P(c_i|p_j)[\pi (p_j) \prod_{k=\text{children indices of }p_j}\lambda_k(p_j)]
                \end{align}
        \end{itemize}
    \item Using \(\lambda\) and \(\pi\) evidence, one may transmit messages up and down the network from the instantiated nodes to find evidence for any node in the network.
    \item \textbf{Notation}
        \begin{itemize}
            \item  \(\vec{\lambda_F(c)} \) means \(\lambda\) evidence from node F for node C.
            \item  \(\vec{\pi_F(c)}\) means \(\pi\) evidence from \textit{all nodes besides} F for node C.
        \end{itemize}
\end{itemize}

\section{Probability Propagation in Singly Connected Networks}

\begin{itemize}
    \item \textbf{Singly connected} means that there is a \textit{single} path between any two nodes in the network. 
    \item The main properties of these networks are:
        \begin{itemize}
            \item The parents of a node are always independent given their common child (i.e. they don't have a common parent). This lets us calculate their joint probability as the product of their marginals.
            \item When updating evidence of a node, the belief propagated through the net to update all nodes is guaranteed to reach a steady state and not loop forever.
        \end{itemize}
    \item An example of a link matrix for a node with multiple parents looks as follows:
        \begin{align}
            \mat{P(e|w,c)} = \begin{bmatrix} P(e_1|w_1,c_1) & P(e_1|w_1,c_2) & P(e_1|w_2,c_1) & P(e_1|w_2,c_2)\\ P(e_2|w_1,c_1) & P(e_2|w_1,c_2) & P(e_2|w_2,c_1) & P(e_2|w_2,c_2) \\ P(e_3|w_1,c_1) & P(e_3|w_1,c_2) & P(e_3|w_2,c_1) & P(e_3|w_2,c_2) \end{bmatrix}
        \end{align}
    \item To calculate the \(\pi\) evidence of a node with 2 parents: 
        \begin{align} 
            \pi(\mat{E}) = \mat{P(e|w,c)}\mat{\pi_e(w,c)} = \mat{P(e|w,c)}\mat{\pi_e(w)\pi_e(c)}
        \end{align}
    \item To calculate the \(\lambda\) evidence of a node with 2 parents, we have to calculate one \(\lambda\) message for each of the parents. If \(c\) has parents \(a\) and \(b\), then the evidence from \(c\) to \(a\) is as follows: 
        \begin{align} 
            \lambda_c(a_i) = \sum_{j=1}^n\pi_{c}\vec{c}(b_j)\sum_{k=1}^m P(c_k|a_i,b)\lambda(c_k)
        \end{align}

        where \(n\) is the number of values that \(b\) takes, and \(m\) is the number of values \(c\) takes. 
    \item \textbf{Operations}
        \begin{itemize}
            \item A and B are the parents of C
            \item \textbf{\(\lambda\) message} (C to A)
                \begin{align} 
                    \lambda_c(a_i) = \sum_{j=1}^n\pi_{c}\vec{c}(b_j)\sum_{k=1}^m P(c_k|a_i,b)\lambda(c_k)
                \end{align}
            \item \textbf{\(\pi\) message} (A to C)
                \begin{align}
					  \pi_c(a_i)=\begin{cases}
                                   1\qquad &\text{if A is instantiated to }a_k\\
                                   0 \qquad &\text{if A is instantiated but not at }a_k \\
                                   P'(a_i)/\lambda_c(a_i) \qquad &\text{if C is not instantiated}
								\end{cases}
                \end{align}
            \item \textbf{\(\lambda\) evidence}
                \begin{align}
					  \lambda(c_k)=\begin{cases}
                                   1\qquad &\text{if C is instantiated to }c_k\\
                                   0 \qquad &\text{if C is instantiated but not at }c_k \\
                                   \prod_i \lambda_{d_i}(c_k) \qquad &\text{if C is not instantiated}
								\end{cases}
                \end{align}
            \item \textbf{\(\pi\) evidence} (A and B to C)
                \begin{align}
                    \pi(c_k) = \sum_i\sum_jp(c_k|a_i,b_j)\pi_c(a_i)\pi_c(b_j)
                \end{align}
            \item \textbf{Posterior probability}
                \begin{align}
                    P'(c_k)=\alpha \pi(c_k)\lambda(c_k)
                \end{align}
        \end{itemize}
\end{itemize}

\end{document}
